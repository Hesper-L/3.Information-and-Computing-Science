## 学习笔记|项目记录（算法组）

### 数学题（Day1）

关于求导数的题目，翻阅《高等代数》一书，巩固了有关指数函数求导，可以换元，接着通过求其对数函数的导函数解决问题；

关于数列收敛问题，翻阅《数学分析》一书，查阅“有界”、“收敛”相关章节，应用“单调有界函数一定收敛”的定理证明题目。

### 数据结构（Day2-Day3）

（发现问题）一开始看到题目是懵圈的，由于python语言的简洁性，决定用python解决问题。

Python本来是0基础，刚好在任务发布之前听完了慕课里面北工大的老师所上的57小节网课，对python的全局大观有了一定的了解。

（分析问题）其实就解决问题,我已经有以下思路：

阶梯足够短时，零阶或者一阶会产生最小花费，即:当i<2,f[i]=cost[i];

而有超过二阶时，最小花费就是当前阶梯的花费加上前两阶或者前一阶的花费较小值，即：当i>=2,状态转移方程：f[i]=cost[i]+min(f[i-2],f[i-1])

   (解决问题)而了解到数据结构问题其实就是力扣的书写解答，所以去b站听了一些力扣简单题的分析与解答，初步掌握技巧。由朋友推荐到CSDN网站寻找思路：怎样转化为代码

class Solution:
    def minCostClimbingStairs(self, cost: List[int]) -> int:
        for i in range(2, len(cost)):
            cost[i] = min(cost[i - 2], cost[i - 1]) + cost[i]
        return min(cost[-2], cost[-1])

### 算法实现（Day4-Day14）

#### 低难度

##### 1.模糊c均值聚类算法（FCM）的代码实现以及论文综述

###### 引

一开始对题目的内容概念都比较模糊，介于我是任务驱动型，因而对此深入学习。然后通过CSDN网站进行搜索，有针对性的选择了文章了解详情。而文章中的专有名词则是通过浏览器搜索、查阅了相关文献，明白了内容的大致意思。论文写作基于泛读文章基础之下，边精读边用自己的语言转化输出。框架为：模糊理论-算法原理-算法步骤-python实现-参考文献。

###### 算法原理

我对“模糊”的理解是突破传统数学[0,1]之间的实数来描述中间状态，而采用“隶属度”——推广到0、1之间多个取值来描述元素与几何之间的关系。模糊c-means较k-means比较灵活自然、非概率。

“模糊c均值聚类融合了模糊理论的精髓。相较于k-means的硬聚类，模糊c提供了更加灵活的聚类结果。因为大部分情况下，数据集中的对象不能划分成为明显分离的簇，指派一个对象到一个特定的簇有些生硬，也可能会出错。所以，对每个对象和每个簇赋予一个权值，指明对象属于该簇的程度。当然，基于概率的方法也可以给出这样的权值，但是有时候我们很难确定一个合适的统计模型，因此使用具有自然地、非概率特性的模糊c均值就是一个比较好的选择。”

简言之，就是要最小化目标函数Jm:（在一些资料中也定义为SSE即误差的平方和）

J~m~ ^m^$\Sigma$ ~i=1~ ^c^$\Sigma$ ~j=1~ u~ij~^m^ ||x~i~-c~j~||^2^

其中m是聚类的簇数；i，j是类标号；表示样本属于j类的隶属度。i表示第i个样本，x是具有d维特征的一个样本。是j簇的中心，也具有d维度。||*||可以是任意表示距离的度量。

模糊c是一个不断迭代计算隶属度和簇中心的过程，直到他们达到最优。

(公式图上传至warehouse库)
2 minutes ago

[image202103222](https://github.com/Hesper-L/warehouse/blob/main/image20210322210719.png)

[image202103222](https://github.com/Hesper-L/warehouse/blob/main/image20210322210734.png)

注：对于单个样本，它对于每个簇的隶属度之和为1。

迭代的终止条件为：

(公式图上传至warehouse库)

![img](https://raw.githubusercontent.com/Hesper-L/warehouse/main/image20210322210807.png

其中k是迭代步数，是误差阈值。上式含义是，继续迭代下去，隶属程度也不会发生较大的变化。即认为隶属度不变了，已经达到比较优（局部最优或全局最优）状态了。该过程收敛于目标Jm的局部最小值或鞍点。

###### 算法目的

通过优化目标函数得到每个样本点对所有类中心的隶属度，从而决定样本点的类属以达到自动对样本数据进行分类的目的。

###### 算法实现步骤

(公式图上传至warehouse库)

![img](https://raw.githubusercontent.com/Hesper-L/warehouse/main/image20210322213353.png)

1、初始化

通常采用随机初始化。即权值随机地选取。簇数需要人为选定。

2、计算质心

FCM中的质心有别于传统质心的地方在于，它是以隶属度为权重做一个加权平均。

3、更新模糊伪划分

即更新权重（隶属度）。简单地说，如果x越靠近质心c，则隶属度越高，反之越低。

###### 算法优缺点

[优] 模糊聚类分析作为无监督机器学习的主要技术之一，是用模糊理论对重要数据分析和建模的方法，建立了样本类属的不确定性描述，能比较客观地反映现实世界，它已经有效地应用在大规模数据分析、数据挖掘、矢量量化、图像分割、模式识别等领域，具有重要的理论与实际应用价值。

[缺] 迭代过程时间长；簇数人为选定；多解性

###### Pyhon算法实现【基于iris数据集】

(我跟着敲代码，尝试理解)

Github路径：

[warehouse](https://github.com/Hesper-L/warehouse)/**Fuzzy C-Means的Python算法实现代码.py** 


###### 参考文献或其链接

J. C. Dunn (1973): "A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters", Journal of Cybernetics 3: 32-57

J. C. Bezdek (1981): "Pattern Recognition with Fuzzy Objective Function Algoritms", Plenum Press, New York

Pang-Ning Tan, et al.数据挖掘导论

http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html

https://blog.csdn.net/zwqhehe/article/details/75174918

https://blog.csdn.net/lyxleft/article/details/88964494；



##### 2.拟牛顿法（DFP算法)代码实现以及论文综述

有了对Fuzzy C-Means的论文综述经验，对此有了明确的方向，展开学习。

###### 引

牛顿法属于利用一阶和二阶导数的无约束目标最优化方法。基本思想是，在每一次迭代中，以牛顿方向为搜索方向进行更新。牛顿法对目标的可导性更严格，要求二阶可导，有Hesse矩阵求逆的计算复杂的缺点。XGBoost本质上就是利用牛顿法进行优化的。

牛顿法算法步骤如下：

（1）给定给初始点 x^(0)^，允许误差 ϵ
（2）计算点 x^(t)^ 处梯度g~t~和Hesse矩阵H，若|g~t~|<ϵ,则停止迭代
（3）计算点 x^(t)^ 处的牛顿方向作为搜索方向：
                                 d(t)=−H~t~^−1^g~t~

（4）从点 x^(t)^ 出发，沿着牛顿方向 d^(t)^ 做一维搜索，获得最优步长：
                                 λ~t~=argmin~λ~f(x^(t)^+λ⋅d^(t)^)

（5）更新参数
                                 x^(t+1)^=x^(t)^+λ~t~⋅d^(t)^

###### 算法原理

（1）给定初始点 x^(0)^，允许误差 ϵ，令 D~0~=I~n~（n是x的维数），t=0
（2）计算搜索方向 d^(t)^=−D~t~^−1^⋅g~t~
（3）从点 x^(t)^出发，沿着 d^(t)^ 做一维搜索，获得最优步长并更新参数：
                     λ~t~=argmin~λ~f(x^(t)^+λ⋅d^(t)^)

​                     x^(t+1)^=x^(t)^+λ~t~⋅d^(t)^

（4）判断精度，若 |g~t+1~|<ϵ则停止迭代，否则转（5）
（5）计算 Δg=g~t+1~−g~t~, Δx=x^(t+1)^−x^(t)^，更新 H
                     D~t+1~=D~t~+(ΔxΔx^T^/Δg^T^Δx−DtΔgΔg^T^D~t~/Δg^T^D~t~Δg)

（6） t=t+1，转（2）

###### 算法目的

牛顿法中的Hesse矩阵H在稠密时求逆计算量大，也有可能没有逆（Hesse矩阵非正定）。拟牛顿法提出，用不含二阶导数的矩阵 U~t~替代牛顿法中的 H~t~^-1^，然后沿搜索方向 −U~t~g~t~做一维搜索.根据不同的 U~t~ 构造方法有不同的拟牛顿法。

其目的是不算二阶导及其逆矩阵，设法构造一个矩阵U，用它来逼近H^-1^

###### 算法实现步骤(MATLAB)

（[warehouse](https://github.com/Hesper-L/warehouse)/**dichotomy.m**）

###### 算法优缺点

[优] 不用算二阶导数；不用求逆

[缺] 算法效率低于BFGS算法；G的初始值选取需要近似正定实数矩阵；处理一些规模较大的非二次型问题时，DFP算法会被“卡住”，迭代无法继续展开，原因是  H~k~矩阵接近称为奇异矩阵;公式涉及矩阵乘法，综合复杂度O（n^2^）

###### 参考文献

【博客】LBFGS方法推导-慢慢的回味
【博客】数值优化：理解L-BFGS算法
【博客】无约束优化算法——牛顿法与拟牛顿法（DFP，BFGS，LBFGS）
【博客】无约束最优化方法——牛顿法、拟牛顿法、BFGS、LBFGS
【博客】Numerical Optimization: Understanding L-BFGS
【论文】A Stochastic Quasi-Newton Method for Online Convex Optimization
【书籍】Numeric Optimization

https://blog.csdn.net/google19890102/article/details/45848439；
https://wenku.baidu.com/view/47d5f20af78a6529647d532a.html；

https://max.book118.com/html/2017/0324/96691700.shtm；
https://wenku.baidu.com/view/7263fac2dd3383c4bb4cd26b.html?from=search；

https://blog.csdn.net/appleyuchi/article/details/97395358；

https://blog.csdn.net/songbinxu/article/details/79677948

##### &用于求解目标函数：100*(2*x1*x1+2)*(2*x1*x1+2)+x2^2

(函数图像-https://github.com/Hesper-L/warehouse/blob/main/%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.fig)

(DFP文件上传至github-https://github.com/Hesper-L/warehouse/blob/main/DFP.m)

(**求解函数寻优并输出的结果.docx**-https://github.com/Hesper-L/warehouse/blob/main/%E6%B1%82%E8%A7%A3%E5%87%BD%E6%95%B0%E5%AF%BB%E4%BC%98%E5%B9%B6%E8%BE%93%E5%87%BA%E7%9A%84%E7%BB%93%E6%9E%9C.docx)

[

尝试过很多方法画函数图像，但是都失败了；原因是Z必须是矩阵方阵，以及某些未知数设置不能是向量等。以下是失败案例：

%画函数图像
x1=[-100:10:100];
x2=[-100:10:100];
[X1,X2]=meshgrid(x1,x2);%生成x1,x2网面
Z=100*(2*x1.^2+2).^2+x2.^2;
mesh(X1,X2,Z)
figure
surf(X1,X2,Z)

以下是成功绘图输入：

ezmesh('100*(2*x1*x1+2)*(2*x1*x1+2)+x2*x2')

]

##### 3.主成分分析法（PCA算法）的代码实现以及论文综述

###### 引

在许多研究与应用中，需要收集大量数据进行分析寻找规律，一定程度上增加了数据采集的工作量。而许多变量之间相关性存在是具有可能性的；对单个指标分析是孤立可能损失有用信息。

主成分分析与因子分析是对减少分析指标的同时避免包含信息的损失，达到对所收集数据全面分析的一类降维算法。

###### PCA概念

PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。

得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法。

###### 算法原理

**【特征值分解矩阵原理】**
(1) 特征值与特征向量

如果一个向量v是矩阵A的特征向量，将一定可以表示成下面的形式：

**Av=λv**

其中，λ是特征向量v对应的特征值，一个矩阵的一组特征向量是一组正交向量。
(2) 特征值分解矩阵

对于矩阵A，有一组特征向量v，将这组向量进行正交化单位化，就能得到一组正交单位向量。特征值分解，就是将矩阵A分解为如下式：

**A=QΣQ^-1^**

其中，Q是矩阵A的特征向量组成的矩阵，则是一个对角阵，对角线上的元素就是特征值。

**【SVD分解矩阵原理】**

奇异值分解是一个能适用于任意矩阵的一种分解的方法，对于任意矩阵A总是存在一个奇异值分解：

**A=UΣV^T^**

假设A是一个m*n的矩阵，那么得到的U是一个m*m的方阵，U里面的正交向量被称为左奇异向量。Σ是一个m*n的矩阵，Σ除了对角线其它元素都为0，对角线上的元素称为奇异值。V^T^是v的转置矩阵，是一个n*n的矩阵，它里面的正交向量被称为右奇异值向量。而且一般来讲，我们会将Σ上的值按从大到小的顺序排列。

**SVD分解矩阵A的步骤：**

(1) 求**AA^T^**的特征值和特征向量，用单位化的特征向量构成 U。

(2) 求**A^T^A**的特征值和特征向量，用单位化的特征向量构成 V。

(3) 将**AA^T^**或者**A^T^A**的特征值求平方根，然后构成 Σ。

###### 算法目的

需要找到一种合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。

###### 算法实现步骤

**(1) 基于特征值分解协方差矩阵实现PCA算法**
输入：数据集 X={x~1~,x~2~,x~3~,...,x~n~}，需要降到k维。
1) 去平均值(即去中心化)，即每一位特征减去各自的平均值。

2) 计算协方差矩阵**$\frac{1}{n}$XX^T^**,注：这里除或不除样本数量n或n-1,其实对求出的特征向量没有影响。

3) 用特征值分解方法求协方差矩阵**$\frac{1}{n}$XX^T^**的特征值与特征向量。

4) 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P。

5) 将数据转换到k个特征向量构建的新空间中，即Y=PX。

**(2) 基于SVD分解协方差矩阵实现PCA算法**
输入：数据集X={x~1~,x~2~,x~3~,...,x~n~}，需要降到k维。

1) 去平均值，即每一位特征减去各自的平均值。

2) 计算协方差矩阵。

3) 通过SVD计算协方差矩阵的特征值与特征向量。

4) 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。

5) 将数据转换到k个特征向量构建的新空间中。

在PCA降维中，我们需要找到样本协方差矩阵XX^T^的最大k个特征向量，然后用这最大的k个特征向量组成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵XX^T^,当样本数多、样本特征数也多的时候，这个计算还是很大的。当我们用到SVD分解协方差矩阵的时候，SVD有两个好处：

1) 有一些SVD的实现算法可以先不求出协方差矩阵XX^T^也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解而是通过SVD来完成，这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是特征值分解。

2)注意到PCA仅仅使用了我们SVD的左奇异矩阵，没有使用到右奇异值矩阵，那么右奇异值矩阵有什么用呢？

假设我们的样本是m*n的矩阵X，如果我们通过SVD找到了矩阵X^T^X最大的k个特征向量组成的k*n的矩阵V^T^ ,则我们可以做如下处理：

**X'~m*k~=X~m*n~V~n*k~^T^**

可以得到一个m*k的矩阵X',这个矩阵和我们原来m*n的矩阵X相比，列数从n减到了k，可见对列数进行了压缩。也就是说，左奇异矩阵可以用于对行数的压缩；右奇异矩阵可以用于对列(即特征维度)的压缩。这就是我们用SVD分解协方差矩阵实现PCA可以得到两个方向的PCA降维(即行和列两个方向)。

[python实现]

（[PCA算法的python实现.py](https://github.com/Hesper-L/warehouse/blob/main/PCA算法的python实现.py)）

###### 算法优缺点

[优]使得数据更易使用；降低算法的计算开销；去除噪声；使得结果更易理解；有效表示同一类样本共同特点的主轴方向

[缺]具有一定模糊性，不如原始样本完整；贡献小的主成分可能含有对样本差异的重；不适合用于区分不同的样本类

###### 参考文献

(1) 主成分分析（PCA）原理详解

http://blog.csdn.net/zhongkelee/article/details/44064401

(2) 机器学习之PCA主成分分析 - steed灬 - 博客园

https://www.cnblogs.com/steed/p/7454329.html

(3) 简单易学的机器学习算法——主成分分析(PCA)

https://blog.csdn.net/google19890102/article/details/27969459

(4) 机器学习实战之PCA - 笨鸟多学 - 博客园

https://www.cnblogs.com/zy230530/p/7074215.html

(5) 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园

http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html

(6) 从PCA和SVD的关系拾遗

https://blog.csdn.net/Dark_Scope/article/details/53150883

(7) CodingLabs - PCA的数学原理

http://blog.codinglabs.org/articles/pca-tutorial.html

(8) PCA(主成分分析)python实现

https://www.jianshu.com/p/4528aaa6dc48

(9) 主成分分析PCA（Principal Component Analysis）在sklearn中的应用及部分源码分析

(10)主成分分析（PCA）原理详解

https://www.cnblogs.com/lochan/p/7001907.html
https://blog.csdn.net/program_developer/article/details/80632779